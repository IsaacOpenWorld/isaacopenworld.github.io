<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=default">
    </script>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>Challenge | IsaacOpenWorld</title>
</head>

<body>

    <div class="banner">
        <img src="assets/banner.jpg" alt="Conference Template Banner">
        <div class="top-left">
            <span class="title1">Isaac</span><span class="title2">OpenWorld</span>
        </div>
        <div class="bottom-center">
            Benchmarking Embodied Agents in Open World Environments 
            <br> NeurIPS 2025
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a title="Home Page" href=".">Home</a>
            </td>
            <td class="navigation">
                <a class="current" title="Challenge" href="challenge">Challenge</a>
            </td>
            <td class="navigation">
                <a title="Participate" href="participate">Participate</a>
            </td>
            <td class="navigation">
                <a title="Leaderboard" href="leaderboard">Leaderboard</a> 
            </td>
            <td class="navigation">
                <a title="Organizers" href="organizers">Organizers</a> 
            </td>
        </tr>
    </table>

    <h2>Challenge</h2>
    The DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) program has charted a 
    research agenda aimed at unearthing new insights pertaining to sim-to-sim, sim-to-real, transfer learning across 
    abstractions, meta-learning, and domain-adaptation.
    <br>
    <br>
    In order to enable this research agenda, groups across Industry and Government have partnered to produce a set of tools 
    including new simulators, environments, scenarios, and datasets that we believe should be made available to the 
    broader research community.
    <br>
    <br>
    To this end, we organize a set of concrete open-world mobile-manipulation (OWMM) scenarios that we believe fit 
    the interests of many researchers coming from the areas of robotics, embodied AI, and machine learning as well as 
    researchers currently working on Large Language Models (LLMs), Vision Language Models (VLMs), world models, and 
    Vision Language-Action Models (VLAs) capable of reasoning over long horizons and carefully planning their actions.
    <br>
    <br>
    We expect that top performing solutions to our competitive scenarios will have significant positive transfer to 
    physical platforms. The simulation platform which is based on NVIDIA's newest generation of Physical AI tools within 
    the Omniverse ecosystem enable direct sim-to-real transfer to physical hardware.

    <h2>Resources and Datasets</h2>
    
    The baseline solutions for the TRAIN scenarios will be provided as teleoperated (human-collected) demonstrations 
    that prove feasibility for each task. Note that in general the solutions to scenarios can require a mixture of high-level (slow) reasoning, 
    intermediate level planning, and low-level (fast) control. All datasets, reasoning models, and control policies 
    are downloadable via huggingface hub.
    <br>
    <br>
    The starting kit containing the simulator, assets, APIs, and documentation on the scenarios as well as the required 
    datasets, models, and policies will be released on <u><b>June 1st</b></u>. There will be code stubs so that participants can get 
    started quickly as well as evaluate on VALIDATION set locally. 

    <h2>Evaluation</h2>
    There are two classes of metrics that will act as the performance indicators for the challenge. 

    <br>
    <br>

    <u><b>Task-specific Metrics:</b></u>
    <br>
    Inherently sparse and directly corresponding to the final states expected of the simulated world. 
    An example is bringing a difficult to locate tool from the workshop building to the kitchen and using it to open a stuck cabinet. 
    The metric is simply that the cabinet is past a threshold in its continuous state that is considered "open".

    <br>
    <br>
    <u><b>General Metrics:</b></u>
    <br>
    Continuous signals that are useful for measuring behavior characteristics, irrespective of task completion. 
    These can be measured quantities including time to complete the objective, total distance traversed, 
    semantic alignment of world exploration with the initial prompt, number of self-recoveries, amongst others.

</body>
</html>

